{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using hybrid features\n",
    "> Features: Bag-of-word + N-gram + POS-tag + Entity Types\n",
    "\n",
    "File Name: GKB_Text_Classification_Hybrid\n",
    "\n",
    "Created Date: 1 March 2017\n",
    "\n",
    "Author: Boya Chen\n",
    "\n",
    "Purpose: Machine Learning Experiments and Results Generation (Hybrid features)\n",
    "\n",
    "Enviornment: Python [2.7.10] ; IPython [5.3.0]\n",
    "\n",
    "Dependencies: scikit-learn [0.18.1] ; numpy [1.12.1] ; scipy [0.19.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from time import time\n",
    "from scipy.sparse import csr_matrix\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from collections import Counter, defaultdict\n",
    "from time import time\n",
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Reading CSV and do pre-processing\n",
    "\n",
    "[OUTPUT] labels, t_labels, t_pos, p_labels, p_pos,  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 - Preparing Data]: ... ...\n",
      "[0 - Preparation Completed]: 2.16776990891 s\n"
     ]
    }
   ],
   "source": [
    "# GEO for 'Geolocation' related classes, (First Column Labels)\n",
    "# APP for 'Appearance' related classes, (Second Column Labels)\n",
    "GEO = 0\n",
    "APP = 1\n",
    "TOPIC = GEO # <- Select One Topic\n",
    "\n",
    "# Dataset Files Paths \n",
    "DATASET_PATH = './dataset.txt'\n",
    "CLEANED_SENTENCES_PATH = \"./dataset_cleaned_sentences_93994.csv\"\n",
    "POS_TAGS_PATH = './dataset_pos_tags_93994.csv'\n",
    "ENTITY_OSMINFO_PATH = \"./entities_set.json\"\n",
    "\n",
    "##########################################\n",
    "#  Prepare training and prediction data  #\n",
    "##########################################\n",
    "t1 = time(); print '[0 - Preparing Data]: ... ...'\n",
    "\n",
    "content = []\n",
    "with open(DATASET_PATH, 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        content.append(row)\n",
    "\n",
    "# Reading the list of labels\n",
    "labels = []\n",
    "for item in content:\n",
    "    labels.append(item[TOPIC])\n",
    "\n",
    "# Reading raw sentences\n",
    "sents = []\n",
    "for item in content:\n",
    "    sents.append(item[2])\n",
    "    \n",
    "# Reading Pre-processed and Cleaned Sents\n",
    "cleaned_sents = []\n",
    "with open(CLEANED_SENTENCES_PATH, \"rb\") as f:\n",
    "    csvReader = csv.reader(f)\n",
    "    for row in csvReader:\n",
    "        cleaned_sents.append(row)\n",
    "\n",
    "# Acquire ent for each sentences\n",
    "with open(ENTITY_OSMINFO_PATH, \"rb\") as f:\n",
    "    ent_with_info = json.load(f)    \n",
    "ent_sents = []\n",
    "for item in content:\n",
    "    ent_sents.append(item[3])\n",
    "type_sents = []\n",
    "for item in ent_sents:\n",
    "    try:\n",
    "        type_feat = defaultdict()\n",
    "        for t in ent_with_info[item]['ent_type']:\n",
    "            type_feat[t] = 1\n",
    "        type_sents.append(type_feat)\n",
    "    except:\n",
    "        type_sents.append({})\n",
    "\n",
    "# Reading POS tags (Pre-processed - 93994 lines)\n",
    "pos_sents = []\n",
    "with open(POS_TAGS_PATH, 'rb') as csvfile:\n",
    "    spamreader = csv.reader(csvfile)\n",
    "    for row in spamreader:\n",
    "        pos_sents.append(row)\n",
    "\n",
    "# Splitting the data\n",
    "cut_num = 17000 # | 0~cut_num --> Training; cut_num~end --> Prediction(unlabeled)\n",
    "t_labels = np.array(labels[:cut_num])\n",
    "t_sents = np.array(sents[:cut_num])\n",
    "t_pos_sents = np.array(pos_sents[:cut_num])\n",
    "\n",
    "# Rest of sentences are for prediction\n",
    "p_sents = np.array(sents[cut_num:])\n",
    "p_pos_sents = np.array(pos_sents[cut_num:])\n",
    "\n",
    "print '[0 - Preparation Completed]:', time()-t1, \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !------- Imbalance Classification Handling ---------!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of positive documents: 1016\n",
      "size of negative documents: 1524\n",
      "size of training dataset: 2540\n"
     ]
    }
   ],
   "source": [
    "# Resampling datset in order to handle the dataset imbalance problem\n",
    "\n",
    "index_of_t = [l for l in range(len(t_labels)) if t_labels[l] == \"t\"]\n",
    "index_of_f = [l for l in range(len(t_labels)) if t_labels[l] == \"f\"]\n",
    "# Shuffle the negative documents in order to do random sampling\n",
    "random.shuffle(index_of_f)\n",
    "# Resample from the labeled data\n",
    "index_of_f = index_of_f[:int(len(index_of_t) * 1.5)]\n",
    "index_of_resample = list(sorted(index_of_t + index_of_f))\n",
    "np.random.shuffle(index_of_resample) # randomly shuffle the order\n",
    "t_labels_resample = [t_labels[i] for i in range(len(t_labels)) if i in index_of_resample]\n",
    "\n",
    "print \"size of positive documents:\", len(index_of_t)\n",
    "print \"size of negative documents:\", len(index_of_f)\n",
    "print \"size of training dataset:\", len(t_labels_resample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  [1] BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - Building BOW feature matrix ... ...\n",
      "1 - [DONE] ----------> 6.68109893799 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<17000x60147 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 194240 stored elements in Compressed Sparse Row format>,\n",
       " <2540x60147 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 30759 stored elements in Compressed Sparse Row format>,\n",
       " <76994x60147 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 901587 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def prep_train_date(data):\n",
    "    feature_matrix = [get_BOW(line) for line in data]\n",
    "    # Using Vectorization functions or Scikit-learn\n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset\n",
    "\n",
    "print \"1 - Building BOW feature matrix ... ...\"\n",
    "t0 = time()\n",
    "dataset_1 = prep_train_date(cleaned_sents)\n",
    "t_dataset_1 = dataset_1[:len(t_labels)]\n",
    "p_dataset_1 = dataset_1[len(t_labels):]\n",
    "\n",
    "t_dataset_1_r = csr_matrix([dataset_1[i].toarray()[0] for i in index_of_resample])\n",
    "\n",
    "print \"1 - [DONE] ---------->\", time()-t0, \"s\"\n",
    "\n",
    "t_dataset_1, t_dataset_1_r, p_dataset_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2] N-gram + vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 - Building Bigram Feature Matrix ... ...\n",
      "2 - [DONE] ----------> 77.8475618362 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<17000x656284 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 189809 stored elements in Compressed Sparse Row format>,\n",
       " <2540x656284 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 30482 stored elements in Compressed Sparse Row format>,\n",
       " <76994x656284 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 883730 stored elements in Compressed Sparse Row format>,\n",
       " 17000)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################################################\n",
    "#  Preparing Training Data with Vectorization  #\n",
    "################################################\n",
    "def get_bag_of_ngram(text, n):\n",
    "    ngram_cnt = Counter()\n",
    "    for i in range(len(text)-(n-1)):\n",
    "        ngram_cnt[tuple(text[i:i+n])] += 1\n",
    "    return ngram_cnt\n",
    "\n",
    "def prep_train_date_NGRAM(data, n):\n",
    "    feature_matrix = [get_bag_of_ngram(sent, n) for sent in data]\n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset\n",
    "\n",
    "t0 = time(); print \"2 - Building Bigram Feature Matrix ... ...\"\n",
    "# -- Call the Function (1~2 Minutes)\n",
    "dataset_2 = prep_train_date_NGRAM(cleaned_sents, 2)\n",
    "\n",
    "# Split the dataset into TRAINING and PREDICTION datasets\n",
    "t_dataset_2 = dataset_2[:len(t_labels)]\n",
    "p_dataset_2 = dataset_2[len(t_labels):]\n",
    "t_dataset_2_r = csr_matrix([dataset_2[i].toarray()[0] for i in index_of_resample])\n",
    "\n",
    "print \"2 - [DONE] ---------->\", time()-t0, \"s\"\n",
    "t_dataset_2, t_dataset_2_r, p_dataset_2, len(t_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3] BAG OF POS TAGS - bigram | vectorization \n",
    "\n",
    "In this phase, builiding a feature matrix which contain POS-tags' bigram (\"n\" or gram can be manually defined) information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 - Building POS TAG Dataset ... ...\n",
      "3 - [DONE] ----------> 5.07329511642 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<93994x906 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 1390294 stored elements in Compressed Sparse Row format>,\n",
       " <2540x906 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 37185 stored elements in Compressed Sparse Row format>,\n",
       " <17000x906 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 246710 stored elements in Compressed Sparse Row format>,\n",
       " <76994x906 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 1143584 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function of getting \"bag of pos tags\"\n",
    "def get_ngram_POS(poses, n):\n",
    "    pos_cnt = Counter()\n",
    "    for i in range(len(poses)-(n-1)):\n",
    "        pos_cnt[tuple(poses[i:i+n])] += 1\n",
    "    return pos_cnt\n",
    "    \n",
    "def prep_train_date_POS(data, n):\n",
    "    feature_matrix = [get_ngram_POS(poses, n) for poses in data]\n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)    \n",
    "    return dataset\n",
    "\n",
    "# -- set time break\n",
    "t0 = time(); print '3 - Building POS TAG Dataset ... ...'\n",
    "\n",
    "dataset_3 = prep_train_date_POS(pos_sents, 2)\n",
    "t_dataset_3 = dataset_3[:cut_num]\n",
    "p_dataset_3 = dataset_3[cut_num:]\n",
    "\n",
    "t_dataset_3_r = csr_matrix([dataset_3[i].toarray()[0] for i in index_of_resample])\n",
    "\n",
    "print '3 - [DONE] ---------->', time()-t0, \"s\"\n",
    "\n",
    "dataset_3, t_dataset_3_r, t_dataset_3, p_dataset_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4] Entities Types\n",
    "\n",
    "Using Entity's OSM type as feature to see if it help to improve the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 - Building OSM Types Feature Matrix ... ...\n",
      "4 - [DONE] ----------> 0.617810964584\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<93994x25 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 114223 stored elements in Compressed Sparse Row format>,\n",
       " <2540x25 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 2670 stored elements in Compressed Sparse Row format>,\n",
       " <17000x25 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 16995 stored elements in Compressed Sparse Row format>,\n",
       " <76994x25 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 97228 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prep_train_date_ET(data):\n",
    "    feature_matrix = data\n",
    "    # Vectorization\n",
    "    vectorizer = DictVectorizer()\n",
    "    dataset = vectorizer.fit_transform(feature_matrix)\n",
    "    return dataset\n",
    "\n",
    "t1 = time(); print '4 - Building OSM Types Feature Matrix ... ...'\n",
    "dataset_4 = prep_train_date_ET(type_sents)\n",
    "t_dataset_4 = dataset_4[:cut_num]\n",
    "p_dataset_4 = dataset_4[cut_num:]\n",
    "\n",
    "t_dataset_4_r = csr_matrix([dataset_4[i].toarray()[0] for i in index_of_resample])\n",
    "\n",
    "print '4 - [DONE] ---------->', time()-t1\n",
    "dataset_4, t_dataset_4_r, t_dataset_4, p_dataset_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [F] Merge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<93994x60207 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 2032893 stored elements in COOrdinate format>,\n",
       " <17000x60207 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 358092 stored elements in COOrdinate format>,\n",
       " <2540x60207 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 55476 stored elements in COOrdinate format>,\n",
       " <76994x60207 sparse matrix of type '<type 'numpy.float64'>'\n",
       " \twith 1674801 stored elements in COOrdinate format>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = hstack((dataset_1, dataset_3, dataset_4))\n",
    "\n",
    "t_dataset = hstack((t_dataset_1, t_dataset_3, t_dataset_4))\n",
    "t_dataset_r = hstack((t_dataset_1_r, t_dataset_3_r, t_dataset_4_r))\n",
    "p_dataset = hstack((p_dataset_1, p_dataset_3, p_dataset_4))\n",
    "\n",
    "dataset, t_dataset, t_dataset_r, p_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading entities sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3591 3643\n"
     ]
    }
   ],
   "source": [
    "# Input Entity name to locate the entity section in the whole dataset\n",
    "name_to_node = defaultdict()\n",
    "for key in ent_with_info:\n",
    "    try:\n",
    "        name_to_node[ent_with_info[key]['tag']['name']] = key\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print len(name_to_node.keys()), len(ent_with_info.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Model Validation and Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_results(predictions, classifications):\n",
    "    lab = ['t', 'f']\n",
    "    print \"accuracy\"\n",
    "#     print accuracy_score(classifications, predictions)\n",
    "    print classification_report(classifications, predictions, labels=lab)\n",
    "\n",
    "def error_checking(real_predictions, labels, verbose):\n",
    "    t_t = len([i for i in range(len(real_predictions)) if real_predictions[i] == 't' and labels[i] == 't'])\n",
    "    f_f = len([i for i in range(len(real_predictions)) if real_predictions[i] == 'f' and labels[i] == 'f'])\n",
    "    t_f = len([i for i in range(len(real_predictions)) if real_predictions[i] == 'f' and labels[i] == 't'])\n",
    "    f_t = len([i for i in range(len(real_predictions)) if real_predictions[i] == 't' and labels[i] == 'f'])\n",
    "\n",
    "    print '--Correct--'\n",
    "    print 'True --> True:', t_t\n",
    "    print 'False --> False:', f_f\n",
    "    print ''\n",
    "    print '--Incorrect--'\n",
    "    print 'True --> False:', t_f\n",
    "    print 'False --> True:', f_t\n",
    "    print ''\n",
    "    print '--> Accuracy:', round((t_t+f_f) / float(t_t+t_f+f_t+f_f) , 2)\n",
    "    print '--> Precision:', round(t_t / float(t_t+f_t) , 2 )\n",
    "    print '--> Recall:', round(t_t / float(t_t+t_f), 2 )\n",
    "    print ''\n",
    "    \n",
    "    if len(verbose) == 2:\n",
    "        test_case = [l for l in verbose]\n",
    "        print \"Sentence: \",test_case[0],\"-->\",test_case[1]\n",
    "        print \"-\"*80\n",
    "        for i, sent in enumerate(sents[:len(real_predictions)]):\n",
    "            if real_predictions[i] == test_case[1] and labels[i] == test_case[0]:\n",
    "                print '>', i, '\\t|  ', sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFIER --> LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "accuracy\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          t       0.40      0.31      0.35      1016\n",
      "          f       0.60      0.69      0.64      1524\n",
      "\n",
      "avg / total       0.52      0.54      0.53      2540\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "CLASSIFIER --> RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "accuracy\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          t       0.44      0.13      0.20      1016\n",
      "          f       0.61      0.89      0.72      1524\n",
      "\n",
      "avg / total       0.54      0.59      0.51      2540\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "CLASSIFIER --> KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
      "           weights='uniform')\n",
      "accuracy\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          t       0.39      0.26      0.31      1016\n",
      "          f       0.59      0.72      0.65      1524\n",
      "\n",
      "avg / total       0.51      0.54      0.52      2540\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "############################\n",
    "# Classifiers from Sklearn #\n",
    "############################\n",
    "# # Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf_nb = MultinomialNB()\n",
    "\n",
    "# Support Vector Machine\n",
    "from sklearn import svm\n",
    "clf_svm = svm.LinearSVC(C=0.1)\n",
    "\n",
    "# # --- Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "# # --- Random Foreset Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rfc = RandomForestClassifier()\n",
    "\n",
    "# # --- Bagging Classifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf_bag = BaggingClassifier()\n",
    "\n",
    "# K Neighbors Classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf_knc = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# --- Cross Validation\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "test_clf = [clf_svm, clf_rfc, clf_knc]\n",
    "for clf in test_clf:\n",
    "    print 'CLASSIFIER -->', str(clf)\n",
    "#     print '------- CROSS VALIDATION --------'\n",
    "    crossval_predicted = cross_val_predict(clf, t_dataset_r, t_labels_resample, cv=10)\n",
    "    check_results(crossval_predicted, t_labels_resample)\n",
    "    print '-'*100\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This phase will use the classifiers to train the real-world (unlabelled data)\n",
    "# Fitting the model\n",
    "real_predictions = clf_rfc.fit(t_dataset_r, t_labels_resample).predict(p_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\t\tMount Martha \n",
      "EntityId:\tnode_2607419375\n",
      "\n",
      "[error] the query is not the p rediction section\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a entity and check the results\n",
    "# If sentences are labelled as positve, they will be printed with staring symbol \"[v]\"; \n",
    "# if not, sentences the start symbols will be \"[x]\"\n",
    "\n",
    "e_names = name_to_node.keys()\n",
    "random.shuffle(e_names)\n",
    "print \"Query:\\t\\t\", e_names[0], \"\\nEntityId:\\t\", name_to_node[e_names[0]]\n",
    "chosen_node = name_to_node[e_names[0]]\n",
    "print ''\n",
    "\n",
    "p_content = content[cut_num:]\n",
    "\n",
    "for i in range(len(p_sents)):\n",
    "    cnt = 0\n",
    "    if p_content[i][3] == chosen_node:\n",
    "        cnt += 1\n",
    "        if real_predictions[i] == 'f':\n",
    "            print '[x]', i+cut_num, \"||\",  p_sents[i]\n",
    "        elif real_predictions[i] == 't':\n",
    "            print '\\n', '~' * 80\n",
    "            print '[v]', i+cut_num, \"||\",  p_sents[i]\n",
    "            print '~' * 80\n",
    "            print ''\n",
    "\n",
    "if cnt == 0:\n",
    "    print \"[error] the query is not the p rediction section\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
