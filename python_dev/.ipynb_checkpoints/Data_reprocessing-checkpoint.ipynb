{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "from os import walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# <<SENT>>: Original sentence\n",
    "# <<POSTAG>>: Original sentence with postag\n",
    "# <<PARSER>>: String parse tree of the original sentence\n",
    "# <<DEP_PARSER>>: Tring dependency tree ofthe original sentence\n",
    "tag_idr_regexr = { 'sent': \"(?<=<<SENT>>: ).+\",\n",
    "                   'postag': \"(?<=<<POSTAG>>: ).+\",\n",
    "                   'ner': \"(?<=<<NER>>: ).+\",\n",
    "                   'parser': \"(?<=<<PARSER>>: ).+\",\n",
    "                   'dparser': \"(?<=<<DEP_PARSER>>: ).+\" }\n",
    "\n",
    "# set_sent = [line for line in [re.findall( tag_idr_regexr[\"sent\"], line) for line in content] if len(line)>0]\n",
    "# set_postag = [line for line in [re.findall( tag_idr_regexr[\"postag\"], line) for line in content] if len(line)>0]\n",
    "# set_ner = [line for line in [re.findall( tag_idr_regexr[\"ner\"], line) for line in content] if len(line)>0]\n",
    "# set_parser = [line for line in [re.findall( tag_idr_regexr[\"parser\"], line) for line in content] if len(line)>0]\n",
    "# set_dparser = [line for line in [re.findall( tag_idr_regexr[\"dparser\"], line) for line in content] if len(line)>0]\n",
    "\n",
    "f_list= []\n",
    "_dir = \"../output_all/\"\n",
    "for (dirpath, dirnames, filenames) in walk(_dir):\n",
    "    f_list.extend(filenames)\n",
    "\n",
    "content = []\n",
    "for fName in f_list:\n",
    "    with open(_dir+fName, 'rb') as in_f:\n",
    "        lines = in_f.readlines()\n",
    "        raw_to_sents = [l[0] for l in [re.findall(tag_idr_regexr[\"sent\"], line) for line in lines] if l != []]\n",
    "        content.append(raw_to_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# content_to_line = []\n",
    "# for n, item in enumerate(content):\n",
    "#     content_to_line.extend([[it,f_list[n]] for it in item])\n",
    "#     content_to_line.append([\"\",\"\"])\n",
    "    \n",
    "# for i in range(len(content_to_line)):\n",
    "#     content_to_line[i] = [\"\",\"\"] + content_to_line[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./all_training_sent.csv', 'wb') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(content_to_line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# | Acquire the files with unique content from out_put\n",
    "f_list= []\n",
    "_dir = \"../output_all/\"\n",
    "for (dirpath, dirnames, filenames) in walk(_dir):\n",
    "    f_list.extend(filenames)\n",
    "\n",
    "first_lines_of_files = []\n",
    "unique_content_files_name = []\n",
    "for i in range(len(f_list)):\n",
    "    input_file_path = '../output_all/'+f_list[i]\n",
    "    with open(input_file_path, \"rb\") as input:\n",
    "        head_line = input.readline()\n",
    "    to_sent = re.findall( tag_idr_regexr[\"sent\"], head_line)\n",
    "    if to_sent not in first_lines_of_files:\n",
    "        first_lines_of_files.append(to_sent)\n",
    "        unique_content_files_name.append(f_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3702, 3644)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_list), len(unique_content_files_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 files removed!\n"
     ]
    }
   ],
   "source": [
    "deleted_files_num = 0\n",
    "for fName in f_list:\n",
    "    if fName not in unique_content_files_name:\n",
    "        os.remove('../output_all/'+fName)\n",
    "        deleted_files_num += 1\n",
    "print deleted_files_num, 'files removed!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3702"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(f_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'node_1019733259'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
